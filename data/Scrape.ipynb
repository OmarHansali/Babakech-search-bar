{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import nltk\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the needed nltk models\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk needed models\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and save the data into \"rugs.json\" and into \"rugs-stemmed-description.csv\" with stemmer\n",
    "\n",
    "request = rq.get(url='https://www.babakech.com/kilim-moroccan-rugs?page=3')\n",
    "if request.status_code == 200:\n",
    "    page = bs(request.content, 'html.parser')\n",
    "    items = page.find_all(class_='product_details')\n",
    "\n",
    "    for i in items:\n",
    "        if i.find(class_='product-tag').text[:5].strip() == 'SKU:':\n",
    "            product_url = i.find(class_='product-item-link').get('href')\n",
    "            product_full_url = 'https://www.babakech.com' + product_url\n",
    "            \n",
    "\n",
    "            # url = 'https://www.babakech.com/vintage-rugs/colorful-beni-ouarian-1334.html'\n",
    "            req = rq.get(url=product_full_url)\n",
    "            if req.status_code == 200:\n",
    "                soup = bs(req.content, 'html.parser')\n",
    "                img = soup.find(class_='zoneimage fancybox').get('href')\n",
    "                title = soup.find(class_='product_title').text[:-1].lower()\n",
    "\n",
    "                price = soup.find(id='price-amount').text\n",
    "                component = soup.find(class_='product_short_description')\n",
    "                descriptions = component.find_all('p')\n",
    "                try:\n",
    "                    if descriptions[3].text[1:12] == 'Materials :':\n",
    "                        desc = descriptions[0].text.lower()\n",
    "                        craft = descriptions[2].text.lower()\n",
    "                        reg = descriptions[4].text[9:].lower()\n",
    "                        tri = descriptions[5].text[8:].lower()\n",
    "                        \n",
    "                        materials = descriptions[3].text[13:]\n",
    "                        if materials.count(',') > 0:\n",
    "                            materials_list = materials.split(',')\n",
    "                            mat = [str(item.strip()).lower() for item in materials_list]\n",
    "                        else:\n",
    "                            print('\\n\\nthere is just one material\\n')\n",
    "                            mat = materials.strip().lower()\n",
    "                        \n",
    "                        # Tokenize the description\n",
    "                        description = word_tokenize(desc)\n",
    "                        # Remove stopwords\n",
    "                        stop_words = set(stopwords.words('english'))\n",
    "                        description = [word for word in description if word not in stop_words]\n",
    "                        # Perform stemming\n",
    "                        stemmer = PorterStemmer()\n",
    "                        description = [stemmer.stem(word) for word in description]\n",
    "                        \n",
    "                        with open(\"rugs-stemmed-description.csv\", mode='a', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            if materials.count(',') > 0:\n",
    "                                writer.writerow([' '.join(description) + ' ' + ' '.join(mat) + ' ' + reg + ' ' + tri])\n",
    "                            else:\n",
    "                                writer.writerow([' '.join(description) + ' ' + mat + ' ' + reg + ' ' + tri])\n",
    "\n",
    "                        d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': str(desc), 'craft': str(craft), 'materials': mat, 'region': str(reg), 'tribe': str(tri)}\n",
    "                        with open('rugs.json', 'rb+') as f:\n",
    "                            f.seek(-1, os.SEEK_END)\n",
    "                            f.truncate()\n",
    "                            f.write(b',\\n')\n",
    "                            new_d_json = json.dumps(d_json, indent=4)\n",
    "                            f.write(bytes(new_d_json, 'utf-8'))\n",
    "                            f.write(b'\\n]')\n",
    "\n",
    "                    elif descriptions[2].text[1:12] == 'Materials :':\n",
    "                        craft = descriptions[1].text.lower()\n",
    "                        reg = descriptions[3].text[9:].lower()\n",
    "                        tri = descriptions[4].text[8:].lower()\n",
    "                        \n",
    "                        materials = descriptions[2].text[13:]\n",
    "                        if materials.count(',') > 0:\n",
    "                            materials_list = materials.split(',')\n",
    "                            mat = [str(item.strip()).lower() for item in materials_list]\n",
    "                        else:\n",
    "                            print('\\n\\nthere is just one material\\n')\n",
    "                            mat = materials.strip().lower()\n",
    "                        \n",
    "                        with open(\"rugs-stemmed-description.csv\", mode='a', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            if materials.count(',') > 0:\n",
    "                                writer.writerow([' '.join(mat) + ' ' + reg + ' ' + tri])\n",
    "                            else:\n",
    "                                writer.writerow([mat + ' ' + reg + ' ' + tri])\n",
    "\n",
    "                        d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': None, 'craft': str(craft), 'materials': mat, 'region': str(reg), 'tribe': str(tri)}\n",
    "                        with open('rugs.json', 'rb+') as f:\n",
    "                            f.seek(-1, os.SEEK_END)\n",
    "                            f.truncate()\n",
    "                            f.write(b',\\n')\n",
    "                            new_d_json = json.dumps(d_json, indent=4)\n",
    "                            f.write(bytes(new_d_json, 'utf-8'))\n",
    "                            f.write(b'\\n]')\n",
    "                    \n",
    "                    else:\n",
    "                        print('\\n\\nNo material found!!\\n')\n",
    "\n",
    "                except:\n",
    "                    print(f\"extraction of '{product_full_url}' data faild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and save the data into \"rugs.json\" and into \"rugs-lemmatized-description.csv\" with lemmatizer\n",
    "\n",
    "# https://www.babakech.com/vintage-rugs/\n",
    "# https://www.babakech.com/checkered-rugs/\n",
    "# https://www.babakech.com/abstract-moroccan-rugs/\n",
    "# https://www.babakech.com/abstract-moroccan-rugs?page=2\n",
    "# https://www.babakech.com/abstract-moroccan-rugs?page=3\n",
    "# https://www.babakech.com/abstract-moroccan-rugs?page=4\n",
    "# https://www.babakech.com/kilim-moroccan-rugs/\n",
    "# https://www.babakech.com/kilim-moroccan-rugs?page=2\n",
    "# https://www.babakech.com/kilim-moroccan-rugs?page=3\n",
    "\n",
    "request = rq.get(url='https://www.babakech.com/kilim-moroccan-rugs?page=3')\n",
    "if request.status_code == 200:\n",
    "    page = bs(request.content, 'html.parser')\n",
    "    items = page.find_all(class_='product_details')\n",
    "\n",
    "    for i in items:\n",
    "        if i.find(class_='product-tag').text[:5].strip() == 'SKU:':\n",
    "            product_url = i.find(class_='product-item-link').get('href')\n",
    "            product_full_url = 'https://www.babakech.com' + product_url\n",
    "            \n",
    "\n",
    "            req = rq.get(url=product_full_url)\n",
    "            if req.status_code == 200:\n",
    "                soup = bs(req.content, 'html.parser')\n",
    "                img = soup.find(class_='zoneimage fancybox').get('href')\n",
    "                title = soup.find(class_='product_title').text[:-1].lower()\n",
    "\n",
    "                price = soup.find(id='price-amount').text\n",
    "                component = soup.find(class_='product_short_description')\n",
    "                descriptions = component.find_all('p')\n",
    "                try:\n",
    "                    if descriptions[3].text[1:12] == 'Materials :':\n",
    "                        desc = descriptions[0].text.lower().replace('\\u00a0', ' ').replace('\\u00e9', 'e').replace('\\u2019', '\\'').replace('\\u2014', '-')\n",
    "                        craft = descriptions[2].text.lower()\n",
    "                        reg = descriptions[4].text[9:].lower()\n",
    "                        tri = descriptions[5].text[8:].lower()\n",
    "                        \n",
    "                        materials = descriptions[3].text[13:]\n",
    "                        if materials.count(',') > 0:\n",
    "                            materials_list = materials.split(',')\n",
    "                            mat = [str(item.strip()).lower() for item in materials_list]\n",
    "                        else:\n",
    "                            print('\\n\\nthere is just one material\\n')\n",
    "                            mat = materials.strip().lower()\n",
    "                        \n",
    "                        # Tokenize the description\n",
    "                        description = word_tokenize(desc)\n",
    "                        # Remove stopwords\n",
    "                        stop_words = set(stopwords.words('english'))\n",
    "                        description = [word for word in description if word not in stop_words]\n",
    "                        # Perform lemmatization\n",
    "                        lemmatizer = WordNetLemmatizer()\n",
    "                        words = [lemmatizer.lemmatize(word) for word in description]\n",
    "                        \n",
    "                        with open(\"rugs-lemmatized-description.csv\", mode='a', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            if materials.count(',') > 0:\n",
    "                                writer.writerow([' '.join(description) + ' ' + ' '.join(mat) + ' ' + reg + ' ' + tri])\n",
    "                            else:\n",
    "                                writer.writerow([' '.join(description) + ' ' + mat + ' ' + reg + ' ' + tri])\n",
    "\n",
    "                        d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': str(desc), 'craft': str(craft), 'materials': mat, 'region': str(reg), 'tribe': str(tri)}\n",
    "                        with open('rugs.json', 'rb+') as f:\n",
    "                            f.seek(-1, os.SEEK_END)\n",
    "                            f.truncate()\n",
    "                            f.write(b',\\n')\n",
    "                            new_d_json = json.dumps(d_json, indent=4)\n",
    "                            f.write(bytes(new_d_json, 'utf-8'))\n",
    "                            f.write(b'\\n]')\n",
    "\n",
    "                    elif descriptions[2].text[1:12] == 'Materials :':\n",
    "                        craft = descriptions[1].text.lower()\n",
    "                        reg = descriptions[3].text[9:].lower()\n",
    "                        tri = descriptions[4].text[8:].lower()\n",
    "                        \n",
    "                        materials = descriptions[2].text[13:]\n",
    "                        if materials.count(',') > 0:\n",
    "                            materials_list = materials.split(',')\n",
    "                            mat = [str(item.strip()).lower() for item in materials_list]\n",
    "                        else:\n",
    "                            print('\\n\\nthere is just one material\\n')\n",
    "                            mat = materials.strip().lower()\n",
    "                        \n",
    "                        with open(\"rugs-lemmatized-description.csv\", mode='a', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            if materials.count(',') > 0:\n",
    "                                writer.writerow([' '.join(mat) + ' ' + reg + ' ' + tri])\n",
    "                            else:\n",
    "                                writer.writerow([mat + ' ' + reg + ' ' + tri])\n",
    "\n",
    "                        d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': None, 'craft': str(craft), 'materials': mat, 'region': str(reg), 'tribe': str(tri)}\n",
    "                        with open('rugs.json', 'rb+') as f:\n",
    "                            f.seek(-1, os.SEEK_END)\n",
    "                            f.truncate()\n",
    "                            f.write(b',\\n')\n",
    "                            new_d_json = json.dumps(d_json, indent=4)\n",
    "                            f.write(bytes(new_d_json, 'utf-8'))\n",
    "                            f.write(b'\\n]')\n",
    "                    \n",
    "                    else:\n",
    "                        print('\\n\\nNo material found!!\\n')\n",
    "\n",
    "\n",
    "                except:\n",
    "                    print(f\"extraction of '{product_full_url}' data faild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and save the data into \"bert-rugs.json\" and into \"rugs-bert-description.csv\" with bert\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# https://www.babakech.com/vintage-rugs/\n",
    "# https://www.babakech.com/checkered-rugs/\n",
    "# https://www.babakech.com/abstract-moroccan-rugs/\n",
    "# https://www.babakech.com/abstract-moroccan-rugs?page=2\n",
    "# https://www.babakech.com/abstract-moroccan-rugs?page=3\n",
    "# https://www.babakech.com/abstract-moroccan-rugs?page=4\n",
    "# https://www.babakech.com/kilim-moroccan-rugs/\n",
    "# https://www.babakech.com/kilim-moroccan-rugs?page=2\n",
    "# https://www.babakech.com/kilim-moroccan-rugs?page=3\n",
    "\n",
    "request = rq.get(url='https://www.babakech.com/kilim-moroccan-rugs?page=3')\n",
    "if request.status_code == 200:\n",
    "    page = bs(request.content, 'html.parser')\n",
    "    items = page.find_all(class_='product_details')\n",
    "\n",
    "    for i in items:\n",
    "        if i.find(class_='product-tag').text[:5].strip() == 'SKU:':\n",
    "            product_url = i.find(class_='product-item-link').get('href')\n",
    "            product_full_url = 'https://www.babakech.com' + product_url\n",
    "            \n",
    "\n",
    "            req = rq.get(url=product_full_url)\n",
    "            if req.status_code == 200:\n",
    "                soup = bs(req.content, 'html.parser')\n",
    "                img = soup.find(class_='zoneimage fancybox').get('href')\n",
    "                title = soup.find(class_='product_title').text[:-1].lower()\n",
    "\n",
    "                price = soup.find(id='price-amount').text\n",
    "                component = soup.find(class_='product_short_description')\n",
    "                descriptions = component.find_all('p')\n",
    "                try:\n",
    "                    if descriptions[3].text[1:12] == 'Materials :':\n",
    "                        desc = descriptions[0].text.lower().replace('\\u00a0', ' ').replace('\\u00e9', 'e').replace('\\u2019', '\\'').replace('\\u2014', '-')\n",
    "                        craft = descriptions[2].text.lower()\n",
    "                        reg = descriptions[4].text[9:].lower()\n",
    "                        tri = descriptions[5].text[8:].lower()\n",
    "                        \n",
    "                        materials = descriptions[3].text[13:]\n",
    "                        if materials.count(',') > 0:\n",
    "                            materials_list = materials.split(',')\n",
    "                            mat = [str(item.strip()).lower() for item in materials_list]\n",
    "                            mat = ' '.join(mat)\n",
    "                        else:\n",
    "                            print('\\n\\nthere is just one material\\n')\n",
    "                            mat = materials.strip().lower()\n",
    "                        \n",
    "                        \n",
    "                        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "                        model_bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "                        \n",
    "                        # Tokenize and encode the description\n",
    "                        encoded_desc = bert_tokenizer.encode(desc, add_special_tokens=True)\n",
    "                        tokenized_desc = torch.tensor(encoded_desc).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_desc = model_bert(tokenized_desc)[0][0].mean(dim=0)\n",
    "\n",
    "\n",
    "                        # Tokenize and encode the material\n",
    "                        encoded_mat = bert_tokenizer.encode(mat, add_special_tokens=True)\n",
    "                        tokenized_mat = torch.tensor(encoded_mat).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_mat = model_bert(tokenized_mat)[0][0].mean(dim=0)\n",
    "                        \n",
    "                        \n",
    "                        # Tokenize and encode the region\n",
    "                        encoded_reg = bert_tokenizer.encode(reg, add_special_tokens=True)\n",
    "                        tokenized_reg = torch.tensor(encoded_reg).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_reg = model_bert(tokenized_reg)[0][0].mean(dim=0)\n",
    "                        \n",
    "                        \n",
    "                        # Tokenize and encode tribe\n",
    "                        encoded_tri = bert_tokenizer.encode(tri, add_special_tokens=True)\n",
    "                        tokenized_tri = torch.tensor(encoded_tri).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_tri = model_bert(tokenized_tri)[0][0].mean(dim=0)\n",
    "                        \n",
    "                        description = embedded_desc + embedded_mat + embedded_reg + embedded_tri\n",
    "                        \n",
    "                        description_list = description.cpu().numpy().tolist()\n",
    "\n",
    "                        with open(\"rugs-bert-description.csv\", mode='a', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerow(description_list)\n",
    "                        \n",
    "                        d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': str(desc), 'craft': str(craft), 'materials': mat, 'region': str(reg), 'tribe': str(tri)}\n",
    "                        with open('bert-rugs.json', 'rb+') as f:\n",
    "                            f.seek(-1, os.SEEK_END)\n",
    "                            f.truncate()\n",
    "                            f.write(b',\\n')\n",
    "                            new_d_json = json.dumps(d_json, indent=4)\n",
    "                            f.write(bytes(new_d_json, 'utf-8'))\n",
    "                            f.write(b'\\n]')\n",
    "\n",
    "                    elif descriptions[2].text[1:12] == 'Materials :':\n",
    "                        craft = descriptions[1].text.lower()\n",
    "                        reg = descriptions[3].text[9:].lower()\n",
    "                        tri = descriptions[4].text[8:].lower()\n",
    "                        \n",
    "                        materials = descriptions[2].text[13:]\n",
    "                        if materials.count(',') > 0:\n",
    "                            materials_list = materials.split(',')\n",
    "                            mat = [str(item.strip()).lower() for item in materials_list]\n",
    "                            mat = ' '.join(mat)\n",
    "                        else:\n",
    "                            print('\\n\\nthere is just one material\\n')\n",
    "                            mat = materials.strip().lower()\n",
    "                        \n",
    "                        \n",
    "                        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "                        model_bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "\n",
    "                        # Tokenize and encode the material\n",
    "                        encoded_mat = bert_tokenizer.encode(mat, add_special_tokens=True)\n",
    "                        tokenized_mat = torch.tensor(encoded_mat).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_mat = model_bert(tokenized_mat)[0][0].mean(dim=0)\n",
    "                        \n",
    "                        \n",
    "                        # Tokenize and encode the region\n",
    "                        encoded_reg = bert_tokenizer.encode(reg, add_special_tokens=True)\n",
    "                        tokenized_reg = torch.tensor(encoded_reg).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_reg = model_bert(tokenized_reg)[0][0].mean(dim=0)\n",
    "                        \n",
    "                        \n",
    "                        # Tokenize and encode tribe\n",
    "                        encoded_tri = bert_tokenizer.encode(tri, add_special_tokens=True)\n",
    "                        tokenized_tri = torch.tensor(encoded_tri).unsqueeze(0).to(device)\n",
    "                        \n",
    "                        # Get BERT embeddings\n",
    "                        with torch.no_grad():\n",
    "                            embedded_tri = model_bert(tokenized_tri)[0][0].mean(dim=0)\n",
    "                        \n",
    "                        description = embedded_mat + embedded_reg + embedded_tri\n",
    "                        \n",
    "                        description_list = description.cpu().numpy().tolist()\n",
    "\n",
    "                        with open(\"rugs-bert-description.csv\", mode='a', newline='') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerow(description_list)\n",
    "\n",
    "                        d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': None, 'craft': str(craft), 'materials': mat, 'region': str(reg), 'tribe': str(tri)}\n",
    "                        with open('bert-rugs.json', 'rb+') as f:\n",
    "                            f.seek(-1, os.SEEK_END)\n",
    "                            f.truncate()\n",
    "                            f.write(b',\\n')\n",
    "                            new_d_json = json.dumps(d_json, indent=4)\n",
    "                            f.write(bytes(new_d_json, 'utf-8'))\n",
    "                            f.write(b'\\n]')\n",
    "                    \n",
    "                    else:\n",
    "                        print('\\n\\nNo material found!!\\n')\n",
    "\n",
    "\n",
    "                except:\n",
    "                    print(f\"extraction of '{product_full_url}' data faild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \"rugs.json\" data\n",
    "\n",
    "# d_json = {'image': str(img), 'title': str(title), 'price': int(price), 'description': str(desc), 'craft': str(craft), 'materials': d[0][4], 'region': str(reg), 'tribe': str(tri)}\n",
    "with open('rugs.json', 'r') as f:\n",
    "    print(f.read())\n",
    "    \n",
    "    # for i, line in enumerate(f):\n",
    "    #     if i == 111:\n",
    "    #         print(line)\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \"rugs-lemmatized-description.csv\" data\n",
    "\n",
    "with open(\"rugs-lemmatized-description.csv\", mode='r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall neuspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"dog cat banana\"\n",
    "\n",
    "# Parse the text with spaCy\n",
    "# Our 'document' variable now contains a full parse of the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Using the parsed doc, find similarity between word tokens\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
